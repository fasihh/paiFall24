{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Data science has rapidly become a critical field in today's technology-driven world.\",\n",
       " 'With the ability to analyze vast amounts of data, data scientists are uncovering patterns and insights that drive business decisions, improve efficiencies, and even predict future trends.',\n",
       " 'From finance to healthcare, industries are leveraging data science to enhance decision-making and innovation.',\n",
       " 'Machine learning algorithms, statistical models, and data visualization tools are just a few of the techniques used by data scientists to interpret complex datasets.',\n",
       " 'As data continues to grow exponentially, the demand for skilled data professionals is only expected to rise, making data science one of the most promising career paths of the future.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"\"\"Data science has rapidly become a critical field in today's technology-driven world. With the ability to analyze vast amounts of data, data scientists are uncovering patterns and insights that drive business decisions, improve efficiencies, and even predict future trends. From finance to healthcare, industries are leveraging data science to enhance decision-making and innovation. Machine learning algorithms, statistical models, and data visualization tools are just a few of the techniques used by data scientists to interpret complex datasets. As data continues to grow exponentially, the demand for skilled data professionals is only expected to rise, making data science one of the most promising career paths of the future.\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "sent = sent_tokenize(text)\n",
    "\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drive', 'eat', 'ride', 'travel', 'danc', 'kill', 'die', 'sleep']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['drive', 'eat', 'rid', 'travel', 'dance', 'kill', 'die', 'sleep']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['drives', 'eats', 'rides', 'travels', 'dances', 'kills', 'dies', 'sleeps']\n",
    "\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "display(stemmed)\n",
    "display(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence',\n",
       " 'lot',\n",
       " 'stop',\n",
       " 'words',\n",
       " '.',\n",
       " 'ordinary',\n",
       " 'sentence',\n",
       " 'way',\n",
       " '.',\n",
       " 'among',\n",
       " 'many',\n",
       " 'useful',\n",
       " 'sentences']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "text = 'this is a sentence with a lot of stop words. it is no ordinary sentence by the way. it is among the many useful sentences'\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "[word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"\"\"Data science has rapidly become a critical field in today's technology-driven world. With the ability to analyze vast amounts of data, data scientists are uncovering patterns and insights that drive business decisions, improve efficiencies, and even predict future trends. From finance to healthcare, industries are leveraging data science to enhance decision-making and innovation. Machine learning algorithms, statistical models, and data visualization tools are just a few of the techniques used by data scientists to interpret complex datasets. As data continues to grow exponentially, the demand for skilled data professionals is only expected to rise, making data science one of the most promising career paths of the future.\"\"\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "with open('stemmed.txt', 'w') as f:\n",
    "    f.write(' '.join([stemmer.stem(word) for word in words if word.lower() not in stop_words]))\n",
    "with open('lemmatized.txt', 'w') as f:\n",
    "    f.write(' '.join([lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ['data science rapidly become critical field today technology driven world', 'ability analyze vast amount data data scientist uncovering pattern insight drive business decision improve efficiency even predict future trend', 'finance healthcare industry leveraging data science enhance decision making innovation', 'machine learning algorithm statistical model data visualization tool technique used data scientist interpret complex datasets', 'data continues grow exponentially demand skilled data professional expected rise making data science one promising career path future']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>amount</th>\n",
       "      <th>analyze</th>\n",
       "      <th>become</th>\n",
       "      <th>business</th>\n",
       "      <th>career</th>\n",
       "      <th>complex</th>\n",
       "      <th>continues</th>\n",
       "      <th>critical</th>\n",
       "      <th>data</th>\n",
       "      <th>datasets</th>\n",
       "      <th>decision</th>\n",
       "      <th>demand</th>\n",
       "      <th>drive</th>\n",
       "      <th>driven</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>enhance</th>\n",
       "      <th>even</th>\n",
       "      <th>expected</th>\n",
       "      <th>exponentially</th>\n",
       "      <th>field</th>\n",
       "      <th>finance</th>\n",
       "      <th>future</th>\n",
       "      <th>grow</th>\n",
       "      <th>healthcare</th>\n",
       "      <th>improve</th>\n",
       "      <th>industry</th>\n",
       "      <th>innovation</th>\n",
       "      <th>insight</th>\n",
       "      <th>interpret</th>\n",
       "      <th>learning</th>\n",
       "      <th>leveraging</th>\n",
       "      <th>machine</th>\n",
       "      <th>making</th>\n",
       "      <th>model</th>\n",
       "      <th>one</th>\n",
       "      <th>path</th>\n",
       "      <th>pattern</th>\n",
       "      <th>predict</th>\n",
       "      <th>professional</th>\n",
       "      <th>promising</th>\n",
       "      <th>rapidly</th>\n",
       "      <th>rise</th>\n",
       "      <th>science</th>\n",
       "      <th>scientist</th>\n",
       "      <th>skilled</th>\n",
       "      <th>statistical</th>\n",
       "      <th>technique</th>\n",
       "      <th>technology</th>\n",
       "      <th>today</th>\n",
       "      <th>tool</th>\n",
       "      <th>trend</th>\n",
       "      <th>uncovering</th>\n",
       "      <th>used</th>\n",
       "      <th>vast</th>\n",
       "      <th>visualization</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  algorithm  amount  analyze  become  business  career  complex  \\\n",
       "0        0          0       0        0       1         0       0        0   \n",
       "1        1          0       1        1       0         1       0        0   \n",
       "2        0          0       0        0       0         0       0        0   \n",
       "3        0          1       0        0       0         0       0        1   \n",
       "4        0          0       0        0       0         0       1        0   \n",
       "\n",
       "   continues  critical  data  datasets  decision  demand  drive  driven  \\\n",
       "0          0         1     1         0         0       0      0       1   \n",
       "1          0         0     2         0         1       0      1       0   \n",
       "2          0         0     1         0         1       0      0       0   \n",
       "3          0         0     2         1         0       0      0       0   \n",
       "4          1         0     3         0         0       1      0       0   \n",
       "\n",
       "   efficiency  enhance  even  expected  exponentially  field  finance  future  \\\n",
       "0           0        0     0         0              0      1        0       0   \n",
       "1           1        0     1         0              0      0        0       1   \n",
       "2           0        1     0         0              0      0        1       0   \n",
       "3           0        0     0         0              0      0        0       0   \n",
       "4           0        0     0         1              1      0        0       1   \n",
       "\n",
       "   grow  healthcare  improve  industry  innovation  insight  interpret  \\\n",
       "0     0           0        0         0           0        0          0   \n",
       "1     0           0        1         0           0        1          0   \n",
       "2     0           1        0         1           1        0          0   \n",
       "3     0           0        0         0           0        0          1   \n",
       "4     1           0        0         0           0        0          0   \n",
       "\n",
       "   learning  leveraging  machine  making  model  one  path  pattern  predict  \\\n",
       "0         0           0        0       0      0    0     0        0        0   \n",
       "1         0           0        0       0      0    0     0        1        1   \n",
       "2         0           1        0       1      0    0     0        0        0   \n",
       "3         1           0        1       0      1    0     0        0        0   \n",
       "4         0           0        0       1      0    1     1        0        0   \n",
       "\n",
       "   professional  promising  rapidly  rise  science  scientist  skilled  \\\n",
       "0             0          0        1     0        1          0        0   \n",
       "1             0          0        0     0        0          1        0   \n",
       "2             0          0        0     0        1          0        0   \n",
       "3             0          0        0     0        0          1        0   \n",
       "4             1          1        0     1        1          0        1   \n",
       "\n",
       "   statistical  technique  technology  today  tool  trend  uncovering  used  \\\n",
       "0            0          0           1      1     0      0           0     0   \n",
       "1            0          0           0      0     0      1           1     0   \n",
       "2            0          0           0      0     0      0           0     0   \n",
       "3            1          1           0      0     1      0           0     1   \n",
       "4            0          0           0      0     0      0           0     0   \n",
       "\n",
       "   vast  visualization  world  \n",
       "0     0              0      1  \n",
       "1     1              0      0  \n",
       "2     0              0      0  \n",
       "3     0              1      0  \n",
       "4     0              0      0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from re import sub\n",
    "import pandas as pd\n",
    "\n",
    "text = \"\"\"Data science has rapidly become a critical field in today's technology-driven world. With the ability to analyze vast amounts of data, data scientists are uncovering patterns and insights that drive business decisions, improve efficiencies, and even predict future trends. From finance to healthcare, industries are leveraging data science to enhance decision-making and innovation. Machine learning algorithms, statistical models, and data visualization tools are just a few of the techniques used by data scientists to interpret complex datasets. As data continues to grow exponentially, the demand for skilled data professionals is only expected to rise, making data science one of the most promising career paths of the future.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "res = []\n",
    "for sentence in sentences:\n",
    "    res.append(' '.join([lemmatizer.lemmatize(word) for word in sub(r'[^\\w]', ' ', sentence).lower().split() if word not in stop_words]))\n",
    "\n",
    "cv = CountVectorizer(max_features=100)\n",
    "x = cv.fit_transform(res)\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "df = pd.DataFrame(data=x.toarray(), columns=cv.get_feature_names_out())\n",
    "print(len(res), res)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paiFall24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
