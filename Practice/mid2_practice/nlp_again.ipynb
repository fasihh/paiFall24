{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words/Count Vectorization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>but</th>\n",
       "      <th>enemy</th>\n",
       "      <th>friend</th>\n",
       "      <th>greatest</th>\n",
       "      <th>jim</th>\n",
       "      <th>mean</th>\n",
       "      <th>therefore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   also  and  but  enemy  friend  greatest  jim  mean  therefore\n",
       "0     0    0    0      1       0         0    1     0          0\n",
       "1     1    0    1      1       0         1    1     0          0\n",
       "2     0    1    0      2       2         0    1     1          0\n",
       "3     1    0    1      2       2         0    0     0          0\n",
       "4     0    0    0      1       0         0    1     0          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>but</th>\n",
       "      <th>enemy</th>\n",
       "      <th>friend</th>\n",
       "      <th>greatest</th>\n",
       "      <th>jim</th>\n",
       "      <th>mean</th>\n",
       "      <th>therefore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.478</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.368</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    also    and    but  enemy  friend  greatest    jim   mean  therefore\n",
       "0  0.000  0.000  0.000  0.646   0.000     0.000  0.764  0.000      0.000\n",
       "1  0.478  0.000  0.478  0.282   0.000     0.593  0.334  0.000      0.000\n",
       "2  0.000  0.414  0.000  0.395   0.668     0.000  0.233  0.414      0.000\n",
       "3  0.368  0.000  0.368  0.434   0.735     0.000  0.000  0.000      0.000\n",
       "4  0.000  0.000  0.000  0.383   0.000     0.000  0.453  0.000      0.805"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "Jim enemy\n",
      "But Jim also greatest enemy\n",
      "And enemy enemy friend mean Jim friend\n",
      "But friend enemy friend also enemy\n",
      "Therefore Jim enemy\n",
      "\n",
      "Cosine similarity between sentence 1 and 2: 0.437\n",
      "Cosine similarity between sentence 1 and 3: 0.433\n",
      "Cosine similarity between sentence 1 and 4: 0.28\n",
      "Cosine similarity between sentence 1 and 5: 0.593\n",
      "Cosine similarity between sentence 2 and 3: 0.189\n",
      "Cosine similarity between sentence 2 and 4: 0.474\n",
      "Cosine similarity between sentence 2 and 5: 0.259\n",
      "Cosine similarity between sentence 3 and 4: 0.663\n",
      "Cosine similarity between sentence 3 and 5: 0.257\n",
      "Cosine similarity between sentence 4 and 5: 0.166\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "text = \"Jim is my enemy. But, Jim is also his own greatest enemy. And, if the enemy of my enemy is my friend, then that means Jim is my friend. But, if he's my friend then the enemy of my friend is also my enemy. Therefore, Jim is my enemy!\"\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "res = list(map(' '.join, [[lem.lemmatize(word) for word in word_tokenize(re.sub(r'[^\\w]', ' ', sentence)) if word not in stop_words] for sentence in sentences]))\n",
    "\n",
    "def count_vectorize(corpus):\n",
    "    cv = CountVectorizer(max_features=100)\n",
    "    transform = cv.fit_transform(corpus).toarray()\n",
    "    return pd.DataFrame(columns=cv.get_feature_names_out(), data=transform)\n",
    "\n",
    "def tfidf_vectorize(corpus):\n",
    "    # term frequency (TF) is the amount of times a term 't' appears in a document divided by total terms in the document\n",
    "    # inverse document frequency (IDF) is the whole log of total documents divided by number of documents that contain 't'\n",
    "    # TF-IDF is the product of the two and a high value for it indicates that the term is rare across the entire corpus except the current document\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    transform = tfidf.fit_transform(corpus).toarray()\n",
    "    return np.round(pd.DataFrame(columns=tfidf.get_feature_names_out(), data=transform), 3)\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    return np.round(np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B)), 3)\n",
    "\n",
    "print('Bag of Words/Count Vectorization:')\n",
    "display(count_vectorize(res))\n",
    "print('TF-IDF vectorization:')\n",
    "tfidf_df = tfidf_vectorize(res)\n",
    "display(tfidf_df)\n",
    "\n",
    "print('Sentences:', *res, sep='\\n', end='\\n\\n')\n",
    "for i in range(len(tfidf_df)):\n",
    "    for j in range(i+1, len(tfidf_df)):\n",
    "        similarity = cosine_similarity(tfidf_df.iloc[i].values, tfidf_df.iloc[j].values)\n",
    "        print(f'Cosine similarity between sentence {i+1} and {j+1}: {similarity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine similarities between word pairs:\n",
      "Cosine similarity between \"enemy\" and \"friend\": -0.010999999940395355\n",
      "Cosine similarity between \"enemy\" and \"Jim\": -0.052000001072883606\n",
      "Cosine similarity between \"friend\" and \"Jim\": -0.024000000208616257\n"
     ]
    }
   ],
   "source": [
    "words = list(map(word_tokenize, res))\n",
    "w2v = Word2Vec(words, min_count=1)\n",
    "\n",
    "words_to_compare = ['enemy', 'friend', 'Jim']\n",
    "print('\\nCosine similarities between word pairs:')\n",
    "for i in range(len(words_to_compare)):\n",
    "    for j in range(i + 1, len(words_to_compare)):\n",
    "        similarity = cosine_similarity(w2v.wv[words_to_compare[i]], w2v.wv[words_to_compare[j]])\n",
    "        print(f'Cosine similarity between \"{words_to_compare[i]}\" and \"{words_to_compare[j]}\": {similarity}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paiFall24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
